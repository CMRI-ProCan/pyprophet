plot.stat(t.df)    # nur plots
      t.df.cutoff vs t.df.qvalue
      ersterss: x-label: normalized discriminatnt score cutoff
      zweites : !! "fraction" als y-label

plot.svalue.vs.qvalue(t.df) #    
      t.df.qvalue vs t.df.svalue # leteres ist sensitivity

plot.roc(t.df)

      x-achse: FPR   in t.df.v_x
      y-achse: TPR   in t.df.v_y

      label: AUC     in t.auc


pre.check.data.based.on.main.variable(t.v_score, t.v_class, t.l_lambda = list(TYPE='FIX', LAMBDA=0.4), iostream)

    get.error.stat.from.null (???)
    liefert: df_error ?
             num_alternative
             num_null
             num_total
             AUC

    da geht auch q-value mit ein.

plot.target.decoy.hist(t.v_ds, t.v_class)

    target = known positive
    decoy  = known negative

    barplot, x: "normalized discriminant score"

get.columns(t.df):

    returns hash mit keys ["main"] = "column names ^main_var"
                          ["var"]  = "column names mit ^var"

get.group.rank.vector(t.v_group = c(), t.v_value = c(), t.decreasing=T)

    groups = (   1,   2,   1,   2,   2,   3)
    values = ( 2.0, 1.0, 1.0, 3.0, 0.0, 1.0)

    ->       ( 1  ,   2,   2,   1,   3,   1)

               ^ in gruppe "1" der größte wert
                      ^ in gruppe "w" der zweitgrößte wert
                           ^ in gruppe "1" der zweitgföß0te wert etc


get.breaks(t.v = c(), t.num_bin = 10, t.margin_fraction = 0.05)

    wie "linspace(min-margin, max+margin, n=num_bin)"


process.scores(t.df, reorder, v_col_orcder):

      light_heavy_coelution_score: -198 -> -1
      light_heavy_shape_score:  99 -> 1
      light_heavy_correlation:  -99 -> 0
      etc: intensity_correlation_with_assay
           delta_ratio_sum_light_heavy

      spalten mit nur NA umbennen mit prefix "NA_"

      "wichtige" spalten werden nach vorne permutiert


      return: { "DF" = t.df, "SCORE_COLUMN_NAMES" = allle spaltenname mit scores inkl .main,
              "ALL_NA_SCORE_COLUMN_NAMES = spaltennamen mit nur "NA"}

parse.and.merge

      liest dateien und mergt diese in eine tabelle

get.files

     liest dateien oderr verzeichnisse ein



semi.supervised.classify.and.cross.validate(
        t.df_peak_groups,
        t.num_xval = 1,
        NORMALIZATION.TYPE = 0,
        t.c_norm_ds_Score = "d_score",
        t.c_known_false   = "decoy",
        t.c_tgr = "transition_group_record"

    )
     
    t.c_ds = "LD1"  # scoreing LDA ?
    # x-times cross validation for error stat
    # and: apply mean weights to data

    for (xval_iter in 1:t.num_xval):

        t.l_result <- semi.supervised.clasify(t.df_peak_groups, ...)

        t.df_clfd_pg <- t.l_result["df"]

        # classifier vectors are summed
        t.l_all_classifiers[xval_iter] = single LD1 classifier

        t.df_all_classifiers: append "scalings"

        # noramlisiere mw/var des scoreings:
        # mw/var der top gerankten known negatives !?
        # -> t.df_clfd_pg[, "d_score"]

        t.df_top_cl_pg: spalten t.v_ds, t.v_kf, t.v_test    
        # t.v_ds = top ranked d_sores gruppenweise
        # t.v_kf = known-false-flag 0/1
        # t.v_test = spalte "test" ??ß
        # spaltennanmd: d_score, decoy, test

        t.l_df_top_cl_pg[xval_iter] = t.df_top_cl_pg

   returns:
        last_df = t.df_clfd_pg
        last_result = t.l_result
        df_all_classifiers = gesammelte scalings
        average_classifier = gemittelte ld1 weight vectors
        l_all_classifiers  = list der gelernten LD1 classifiers
        l_df_top_cl_pg     = gesammelte top scores

        





              



split.into.learn.and.test:

          N = anzahl transition groups
          frac = min(0.5 * N, 100)

          t.v_kf_groups: known false gropus namen

          frac davon zufällig ziehen -> t.num_kf_groups_train
          rest:                         t.v_test_kf_groups

          frac bezieht sich auf anzahl der transition groups.

          bei den decoys wohl die ganze group

          unknown_train: analog

          -> t.l["train"] = known_false_train + unkown_train
             known false: 1 .. peak_group_kf_train
             unknwon    : peak_group_kf_train+1 .. peak_group_kf_train+peak_group_unknown_train

             t.l["test"]  = jeweils er rest
             test known false: 1 .. peak_group_kf_test
             unknwon    : peak_group_kf_test+1 .. peak_group_kf_train+peak_group_unknown_test


             "prec_rec_kf_train": frac * # anzahl tgroups known false
             "peak_group_kf_train":      # anzahl known false bspe
             "prec_rec_kf_test":  # anzahl tgroups knwon false zum testen
             "peak_group_kf_test":  # anzahl test bspe knwon false

             "prec_rec_unknown_train": frac * # anzahl tgroups unknwn
             "peak_group_unknown_train":      # anzahl unkknown bspe
             "prec_rec_unknown_test":  # anzahl tgroups unknwon zum testen
             "peak_group_unknown_test":  # anzahl test bspe unknown




innerer em loop:

   - false: top ranked decoys




t.c_pgr = peak group ranking = main score absteigender rank
also pro peak group der beste main score

   

Klasse 0: DECOY
Klasse 1: TARGET
