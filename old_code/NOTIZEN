plot.stat(t.df)    # nur plots
      t.df.cutoff vs t.df.qvalue
      ersterss: x-label: normalized discriminatnt score cutoff
      zweites : !! "fraction" als y-label

plot.svalue.vs.qvalue(t.df) #    
      t.df.qvalue vs t.df.svalue # leteres ist sensitivity

plot.roc(t.df)

      x-achse: FPR   in t.df.v_x
      y-achse: TPR   in t.df.v_y

      label: AUC     in t.auc


pre.check.data.based.on.main.variable(t.v_score, t.v_class, t.l_lambda = list(TYPE='FIX', LAMBDA=0.4), iostream)

    get.error.stat.from.null (???)
    liefert: df_error ?
             num_alternative
             num_null
             num_total
             AUC

    da geht auch q-value mit ein.

plot.target.decoy.hist(t.v_ds, t.v_class)

    target = known positive
    decoy  = known negative

    barplot, x: "normalized discriminant score"

get.columns(t.df):

    returns hash mit keys ["main"] = "column names ^main_var"
                          ["var"]  = "column names mit ^var"

get.group.rank.vector(t.v_group = c(), t.v_value = c(), t.decreasing=T)

    groups = (   1,   2,   1,   2,   2,   3)
    values = ( 2.0, 1.0, 1.0, 3.0, 0.0, 1.0)

    ->       ( 1  ,   2,   2,   1,   3,   1)

               ^ in gruppe "1" der größte wert
                      ^ in gruppe "w" der zweitgrößte wert
                           ^ in gruppe "1" der zweitgföß0te wert etc


get.breaks(t.v = c(), t.num_bin = 10, t.margin_fraction = 0.05)

    wie "linspace(min-margin, max+margin, n=num_bin)"


process.scores(t.df, reorder, v_col_orcder):

      light_heavy_coelution_score: -198 -> -1
      light_heavy_shape_score:  99 -> 1
      light_heavy_correlation:  -99 -> 0
      etc: intensity_correlation_with_assay
           delta_ratio_sum_light_heavy

      spalten mit nur NA umbennen mit prefix "NA_"

      "wichtige" spalten werden nach vorne permutiert


      return: { "DF" = t.df, "SCORE_COLUMN_NAMES" = allle spaltenname mit scores inkl .main,
              "ALL_NA_SCORE_COLUMN_NAMES = spaltennamen mit nur "NA"}

parse.and.merge

      liest dateien und mergt diese in eine tabelle

get.files

     liest dateien oderr verzeichnisse ein



semi.supervised.classify.and.cross.validate(
        t.df_peak_groups,
        t.num_xval = 1,
        NORMALIZATION.TYPE = 0,
        t.c_norm_ds_Score = "d_score",
        t.c_known_false   = "decoy",
        t.c_tgr = "transition_group_record"

    )

    # t.df_peak_groups:=
    #   transition_group_record, decoy, main_var_..., var_...
    #   peak_group_rank_ peak_group_id

    #   transition_group_record setzt sich zusammen aus:
    #     transition_group_id decoy run_id (und decoy_algorithm ?)


    t.c_ds = "LD1"  # scoreing LDA ?
    # x-times cross validation for error stat
    # and: apply mean weights to data

    for (xval_iter in 1:t.num_xval):

        t.l_result <- semi.supervised.clasify(t.df_peak_groups, ...)

        t.df_clfd_pg <- t.l_result["df"]

        # classifier vectors are summed
        t.l_all_classifiers[xval_iter] = single LD1 classifier

        t.df_all_classifiers: append "scalings"

        # noramlisiere mw/var des scoreings:
        # mw/var der top gerankten known negatives !?
        # -> t.df_clfd_pg[, "d_score"]

        t.df_top_cl_pg: spalten t.v_ds, t.v_kf, t.v_test    
        # t.v_ds = top ranked d_sores gruppenweise
        # t.v_kf = known-false-flag 0/1
        # t.v_test = spalte "test" ??ß
        # spaltennanmd: d_score, decoy, test

        t.l_df_top_cl_pg[xval_iter] = t.df_top_cl_pg

   returns:
        last_df = t.df_clfd_pg
        last_result = t.l_result
        df_all_classifiers = gesammelte scalings
        average_classifier = gemittelte ld1 weight vectors
        l_all_classifiers  = list der gelernten LD1 classifiers
        l_df_top_cl_pg     = gesammelte top scores

        





              



split.into.learn.and.test:

          N = anzahl transition groups
          frac = min(0.5 * N, 100)

          t.v_kf_groups: known false gropus namen

          frac davon zufällig ziehen -> t.num_kf_groups_train
          rest:                         t.v_test_kf_groups

          frac bezieht sich auf anzahl der transition groups.

          bei den decoys wohl die ganze group


          unknown_train: analog

          t.l["train"][:peak_group_kf_train] -> decoy records for train
          t.l["train"][peak_group_kf_train:] -> unknown records for train

          t.l["test"][:peak_group_kf_test] -> decoy records for train
          t.l["test"][peak_group_kf_test:] -> unknown records for train


          -> t.l["train"] = known_false_train + unkown_train
             known false: 1 .. peak_group_kf_train
             unknwon    : peak_group_kf_train+1 .. peak_group_kf_train+peak_group_unknown_train

             t.l["test"]  = jeweils er rest
             test known false: 1 .. peak_group_kf_test
             unknwon    : peak_group_kf_test+1 .. peak_group_kf_train+peak_group_unknown_test


             "prec_rec_kf_train": frac * # anzahl tgroups known false
             "peak_group_kf_train":      # anzahl known false bspe
             "prec_rec_kf_test":  # anzahl tgroups knwon false zum testen
             "peak_group_kf_test":  # anzahl test bspe knwon false

             "prec_rec_unknown_train": frac * # anzahl tgroups unknwn
             "peak_group_unknown_train":      # anzahl unkknown bspe
             "prec_rec_unknown_test":  # anzahl tgroups unknwon zum testen
             "peak_group_unknown_test":  # anzahl test bspe unknown




innerer em loop:

   - false: top ranked decoys




t.c_pgr = peak group ranking = main score absteigender rank
also pro peak group der beste main score

   

Klasse 0: DECOY
Klasse 1: TARGET

-------

INI:

$type [1] "initialize"
$t_type [1] "FIX"
# $mm_fdr [1] 0.05
# $kfdist_fdr [1] 0.15
$lambda_parameterize_null [1] 0.4
$num_cutoff [1] 41
# $mm_corr_weight [1] TRUE
$fraction [1] 0.05
$absolute [1] 2
$separation_column "main_var_xx_swath_prelim_score"

$classification_columns 
 [1] "var_bseries_score"             "var_elution_model_fit_score"  
 [3] "var_intensity_score"           "var_isotope_correlation_score"
 [5] "var_isotope_overlap_score"     "var_library_corr"             
 [7] "var_library_rmsd"              "var_log_sn_score"             
 [9] "var_massdev_score"             "var_massdev_score_weighted"   
[11] "var_norm_rt_score"             "var_xcorr_coelution"          
[13] "var_xcorr_coelution_weighted"  "var_xcorr_shape"              
[15] "var_xcorr_shape_weighted"      "var_yseries_score"            

$ds_column [1] "LD1"

-------


$type [1] "iteration"
$max_iter [1] 6
# $convergence [1] 0.05
$t_type [1] "FIX" 
$mm_fdr [1] 0.09 
$kfdist_fdr [1] 0.02 
$lambda_parameterize_null [1] 0.4 
$num_cutoff [1] 41 
$mm_corr_weight [1] TRUE 
$fraction [1] 0.08 
$absolute [1] 2 
$separation_column [1] "LD1" 
$classification_columns [1] NA 
 [1] "main_var_xx_swath_prelim_score" "var_bseries_score"             
 [3] "var_elution_model_fit_score"    "var_intensity_score"           
 [5] "var_isotope_correlation_score"  "var_isotope_overlap_score"     
 [7] "var_library_corr"               "var_library_rmsd"              
 [9] "var_log_sn_score"               "var_massdev_score"             
[11] "var_massdev_score_weighted"     "var_norm_rt_score"             
[13] "var_xcorr_coelution"            "var_xcorr_coelution_weighted"  
[15] "var_xcorr_shape"                "var_xcorr_shape_weighted"      
[17] "var_yseries_score"             
$ds_column [1] "LD1" 

---> bei ini andere separation_column als während der iterationen !
     nach ini ist main_score auch ein feature !

t.df_peak_groups: names:

 [1] "transition_group_record"        "decoy"                         
 [3] "main_var_xx_swath_prelim_score" "var_bseries_score"             
 [5] "var_elution_model_fit_score"    "var_intensity_score"           
 [7] "var_isotope_correlation_score"  "var_isotope_overlap_score"     
 [9] "var_library_corr"               "var_library_rmsd"              
[11] "var_log_sn_score"               "var_massdev_score"             
[13] "var_massdev_score_weighted"     "var_norm_rt_score"             
[15] "var_xcorr_coelution"            "var_xcorr_coelution_weighted"  
[17] "var_xcorr_shape"                "var_xcorr_shape_weighted"      
[19] "var_yseries_score"              "peak_group_rank"               
[21] "peak_group_id"                 


peak_group_id: transition_group_record + laufende nummer innherhalb group

NORMALIZATION.TYPE = 1
t.c_norm_ds_score = "d_score"

t.l_train_test:  type="fraction", fraction=0.5, max_train=150


NACH XVAL:

# ONLY FOR DEBUG:
	t.df_xval_summed <- t.l_ssl_result[["df_all_xval_summed"]]
	
# KLAR:
	t.final_classifier <- t.l_ssl_result[["average_classifier"]]
	
# LISTE, element pro x-eval: tablle mit spalten d_sore, decoy, test
#        d_score: nomralized fisher score
#        decoy: TRUE/FALSE
#        test: 0/1
#   
	t.l_df_top_cl_pg <- t.l_ssl_result[["l_df_top_cl_pg"]]




get.frac.train: -> returns 0.5

--------------------------------------------------------------------------

POSTITIVE EXAMPLES: Zeilen 1973 - 2000

     err_table = get_error_stat_from_null 
     df_stat convert_to_specific_q_value_stat(err_table, q=kfdist_fdr (config), 8)
     # finde zeile mit q_value ~ kfdist_fdr: cutoff ist wert in der zeile !
     cutoff = df_stat[1, "cutoff"]

     true: main_score >= cutoff und kein decoy


def get_error_stat_from_null(main_score, is_decoy, lam=0.5):

     main_score_null = main_scores[is_decoy]
     main_score_target = main_socres[!is_decoy]
     n = len(main_score)
     main_score_target = sorted(main_score_target[main_score_target != nan])

     mu = mean(main_score_null)
     nu = std(main_score_null)
     p_values_target = 1-pnorm(main_score_target, mu, nu)

     err_ = get_error_table_from_pvalues_new(p_values_target, lam)

     return   v_target_pvalue = p_values_target,
              num_total = errt["num"]
              num_alternative = errt["num_alternative"]
              num_null = errt["num_null"]
              df_error = errt["df"] + spalte "cutoff" = main_score_target
                                             ^^^^^^^:w


def get_error_table_from_pvalues_new(p_values, la=0.5):

    num, num_null, num_alternative = estimate_num_null(p_values, la) # storeys method !

    p_values = sorted(p_values, decreasing)

    for i, pi in enumerate(p_values):
        num_positives[i] = sum(p_values <= pi)
        num_negatives[i] = num - num_positives[i]
        pp[i] = num_positives[i] / num # falls num > 0 else: 0
        true_positives[i] = num_positives[i] - num_null * pi
        false_positives[i] = num_null * pi
        true_negatives[i] = num_null * (1-pi)
        false_negatives[i] = num_negatives[i] - true_negatives[i]

        fdr[i] = false_positives[i] / num_positives[i] or 0
        fdr[i] = min(max(fdr, 0), 1) # restrict to 0...1
        sens[i] = true_positives[i] / num_alternative
        sens[i] = min(max(sens, 0), 1) # restrict to 0...1

        q[i] = min(fdr[1:i], na.rm=T) or 1

        fpr[i] = false_positives[i] / num_null
        + cutoff (0,1)

    for i, pi in enumerate(reverse(p_values)):
        svalue[i] = max(reversed(sens)[1:i], na.rm=T)

    svalue[i] = reverse(svalues]


def convert_to_specific_q_value_stat(err_table, q=kfdist_fdr (config), 8):






